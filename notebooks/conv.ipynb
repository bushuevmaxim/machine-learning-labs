{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.insert(1, '/Users/max/projects/machine-learning-labs/tensorflow')\n",
    "from mytensorflow import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfdt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.layers import Dense, Flatten, Reshape, Input, InputLayer\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from  matplotlib import pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "from tensorflow import keras\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH=200\n",
    "IMG_HEIGHT=200\n",
    "img_folder=r'/Users/max/projects/machine-learning-labs/data/catsdogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_dataset_PIL(img_folder):\n",
    "    \n",
    "    img_data_array=[]\n",
    "    class_name=[]\n",
    "    for dir1 in os.listdir(img_folder):\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\n",
    "       \n",
    "            image_path= os.path.join(img_folder, dir1,  file)\n",
    "            image= np.array(Image.open(image_path))\n",
    "            image= np.resize(image,(IMG_HEIGHT,IMG_WIDTH,3))\n",
    "            image = image.astype('float32')\n",
    "            image /= 255  \n",
    "            img_data_array.append(image)\n",
    "            class_name.append(dir1)\n",
    "    return img_data_array , class_name  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL_img_data, class_name=create_dataset_PIL(img_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict={k: v for v, k in enumerate(np.unique(class_name))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_val=  [target_dict[class_name[i]] for i in range(len(class_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (200,200) (199,199) (200,200) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m net\u001b[39m.\u001b[39madd(ActivationLayer(Sigmoid\u001b[39m.\u001b[39mcallFunction, Sigmoid\u001b[39m.\u001b[39mcallDerivative))\n\u001b[1;32m      6\u001b[0m net\u001b[39m.\u001b[39mcompile(BinaryCrossEntropy\u001b[39m.\u001b[39mfun, BinaryCrossEntropy\u001b[39m.\u001b[39mderivative)\n\u001b[0;32m----> 7\u001b[0m net\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49marray(PIL_img_data[::\u001b[39m1000\u001b[39;49m]), tf\u001b[39m.\u001b[39;49mcast(\u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39mint\u001b[39;49m,target_val[::\u001b[39m1000\u001b[39;49m])),tf\u001b[39m.\u001b[39;49mint32), epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n",
      "File \u001b[0;32m~/projects/machine-learning-labs/tensorflow/src/neuro_network.py:39\u001b[0m, in \u001b[0;36mNetwork.fit\u001b[0;34m(self, x_train, y_train, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     37\u001b[0m error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_prime(y_train[j], output)\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m---> 39\u001b[0m     error \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward_propagation(error, learning_rate)\n",
      "File \u001b[0;32m~/projects/machine-learning-labs/tensorflow/src/layers/сonvolutional_alyer.py:39\u001b[0m, in \u001b[0;36mConvolutionalLayer.backward_propagation\u001b[0;34m(self, output_error, learning_rate)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_depth):\n\u001b[1;32m     38\u001b[0m     \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_depth):\n\u001b[0;32m---> 39\u001b[0m         input_error[:, :, d] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m convolve2d(\n\u001b[1;32m     40\u001b[0m             output_error[:, :, k], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[:, :, d, k], \u001b[39m'\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m         dWeights[:, :, d, k] \u001b[39m=\u001b[39m correlate2d(\n\u001b[1;32m     42\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput[:, :, d], output_error[:, :, k], \u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m     dBias[k] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_depth \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(output_error[:, :, k])\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (200,200) (199,199) (200,200) "
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "net.add(ConvolutionalLayer((200,200,3), (3,3), 1))\n",
    "net.add(FlattenLayer())\n",
    "net.add(FCLayer(39204, 1))\n",
    "net.add(ActivationLayer(Sigmoid.callFunction, Sigmoid.callDerivative))\n",
    "net.compile(BinaryCrossEntropy.fun, BinaryCrossEntropy.derivative)\n",
    "net.fit(np.array(PIL_img_data[::1000]), tf.cast(list(map(int,target_val[::1000])),tf.int32), epochs=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.49717491])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = tf.keras.preprocessing.image.load_img(\n",
    "    \"../data/catsdogs/cats/0.jpg\", target_size=(200, 200)\n",
    ")\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "predictions = net.predict(img_array)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
